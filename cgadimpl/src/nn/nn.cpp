#include "nn/nn.hpp"
#include <cmath>
#include <cassert>

namespace ag::nn {

// --- helpers ---
static inline Tensor broadcast_col(const Tensor& v, int R, int C){
    assert(v.cols()==1 && v.rows()==R);
    Tensor y(R,C);
    for (int i=0;i<R;++i){ float vi=v(i,0); for(int j=0;j<C;++j) y(i,j)=vi; }
    return y;
}

// --- elementwise ---
Tensor relu(const Tensor& x){
    Tensor y(x.rows(), x.cols());
    for(int i=0;i<x.rows();++i) for(int j=0;j<x.cols();++j)
        y(i,j) = x(i,j)>0.f ? x(i,j) : 0.f;
    return y;
}
Tensor leaky_relu(const Tensor& x, float a){
    Tensor y(x.rows(), x.cols());
    for(int i=0;i<x.rows();++i) for(int j=0;j<x.cols();++j){
        float v=x(i,j); y(i,j) = v>0.f ? v : a*v;
    }
    return y;
}
Tensor sigmoid(const Tensor& x){
    Tensor y(x.rows(), x.cols());
    for(int i=0;i<x.rows();++i) for(int j=0;j<x.cols();++j){
        float v=x(i,j); y(i,j)=1.f/(1.f+std::exp(-v));
    }
    return y;
}
Tensor tanh(const Tensor& x){
    Tensor y(x.rows(), x.cols());
    for(int i=0;i<x.rows();++i) for(int j=0;j<x.cols();++j)
        y(i,j)=std::tanh(x(i,j));
    return y;
}
Tensor softplus(const Tensor& x){
    Tensor y(x.rows(), x.cols());
    for(int i=0;i<x.rows();++i) for(int j=0;j<x.cols();++j){
        float v=x(i,j);
        y(i,j)=std::log1pf(std::exp(-std::abs(v))) + std::max(v,0.f); // stable softplus
    }
    return y;
}
Tensor silu(const Tensor& x){
    Tensor y(x.rows(), x.cols());
    for(int i=0;i<x.rows();++i) for(int j=0;j<x.cols();++j){
        float v=x(i,j); float s=1.f/(1.f+std::exp(-v)); y(i,j)=v*s;
    }
    return y;
}
Tensor gelu(const Tensor& x){
    Tensor y(x.rows(), x.cols());
    constexpr float c = 0.7978845608028654f; // sqrt(2/pi)
    for(int i=0;i<x.rows();++i) for(int j=0;j<x.cols();++j){
        float v=x(i,j);
        float u=c*(v+0.044715f*v*v*v);
        y(i,j)=0.5f*v*(1.f+std::tanh(u));
    }
    return y;
}

// --- rowwise reductions / softmax ---
Tensor row_max(const Tensor& x){
    Tensor m(x.rows(),1);
    for(int i=0;i<x.rows();++i){
        float mm=x(i,0);
        for(int j=1;j<x.cols();++j) mm = mm>x(i,j)?mm:x(i,j);
        m(i,0)=mm;
    }
    return m;
}
Tensor row_sum(const Tensor& x){
    Tensor s(x.rows(),1);
    for(int i=0;i<x.rows();++i){
        float a=0.f; for(int j=0;j<x.cols();++j) a+=x(i,j);
        s(i,0)=a;
    }
    return s;
}
Tensor logsumexp_row(const Tensor& x){
    Tensor m = row_max(x);                   // [B,1]
    Tensor xm = x - broadcast_col(m, x.rows(), x.cols());
    Tensor e(x.rows(), x.cols());
    for(int i=0;i<x.rows();++i) for(int j=0;j<x.cols();++j) e(i,j)=std::exp(xm(i,j));
    Tensor s = row_sum(e);                   // [B,1]
    Tensor out(s.rows(), s.cols());
    for(int i=0;i<s.rows();++i) out(i,0)=std::log(s(i,0)) + m(i,0);
    return out;
}
Tensor softmax_row(const Tensor& x){
    Tensor m = row_max(x);
    Tensor xm = x - broadcast_col(m, x.rows(), x.cols());
    Tensor e(x.rows(), x.cols());
    for(int i=0;i<x.rows();++i) for(int j=0;j<x.cols();++j) e(i,j)=std::exp(xm(i,j));
    Tensor s = row_sum(e);
    Tensor sb = broadcast_col(s, x.rows(), x.cols());
    Tensor y(x.rows(), x.cols());
    for(int i=0;i<x.rows();++i) for(int j=0;j<x.cols();++j) y(i,j)=e(i,j)/sb(i,j);
    return y;
}

// --- loss ---
Tensor cross_entropy_with_logits(const Tensor& Z, const Tensor& Y){
    assert(Z.rows()==Y.rows() && Z.cols()==Y.cols());
    Tensor lse = logsumexp_row(Z);                 // [B,1]
    Tensor Zm  = Z - broadcast_col(lse, Z.rows(), Z.cols());
    // sum(Y * log_softmax(Z), axis=1)
    Tensor rs = row_sum( Zm * Y );                 // [B,1]
    // mean over batch and negate
    float sum = 0.f; for(int i=0;i<rs.rows();++i) sum += rs(i,0);
    Tensor out(1,1); out(0,0) = -sum / float(Z.rows());
    return out;
}

} // namespace ag::nn
